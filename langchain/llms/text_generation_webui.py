"""Wrapper around NLPCloud APIs."""
from typing import Any, List, Mapping, Optional

import requests
from pydantic import BaseModel, Extra

from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens


class TextGenerationWebUI(LLM, BaseModel):
    """Wrapper around TextGenerationWebUI APIs.

    Example:
        .. code-block:: python

            from langchain.llms import NLPCloud
            nlpcloud = NLPCloud(model="gpt-neox-20b")
    """

    client: Any  #: :meta private:
    model_name: str = "text-generation-webui"
    """Model name to use."""

    max_length: int = 512
    """The maximum number of tokens to generate in the completion."""
    do_sample: bool = True
    """Whether to use sampling (True) or greedy decoding."""
    temperature: float = 0.5
    """What sampling temperature to use."""
    top_p: int = 0.95
    """Total probability mass of tokens to consider at each step."""

    typical_p: int = 0.5
    """Local typicality measures how similar the conditional probability of 
    predicting a target token next is to the expected conditional probability of
    predicting a random token next, given the partial text already generated."""

    repetition_penalty: float = 1.2
    """Penalizes repeated tokens. 1.0 means no penalty."""
    encoder_repetition_penalty: float = 1.0
    """Penalizes repeated tokens. 1.0 means no penalty."""
    top_k: int = 50
    """The number of highest probability vocabulary tokens to keep 
    for top-k-filtering.."""
    min_length: int = 1
    """The minimum number of tokens to generate in the completion."""
    no_repeat_ngram_size: int = 0
    """TODO"""
    num_beams: int = 1
    """Number of beams for beam search."""
    penalty_alpha: int = 1
    """Number of beams for beam search."""

    length_penalty: float = 1.0
    """Exponential penalty to the length."""
    early_stopping: bool = False
    """Whether to stop beam search at num_beams sentences."""

    endpoint: str = ""

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @property
    def _default_params(self) -> Mapping[str, Any]:
        """Get the default parameters for calling NLPCloud API."""
        return {
            "max_length": self.max_length,
            "do_sample": self.do_sample,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "typical_p": self.typical_p,
            "repetition_penalty": self.repetition_penalty,
            "encoder_repetition_penalty": self.encoder_repetition_penalty,
            "top_k": self.top_k,
            "min_length": self.min_length,
            "no_repeat_ngram_size": self.no_repeat_ngram_size,
            "num_beams": self.num_beams,
            "penalty_alpha": self.penalty_alpha,
            "length_penalty": self.length_penalty,
            "early_stopping": self.early_stopping,
        }

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {**{"model_name": self.model_name}, **self._default_params}

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "text-generation-webui"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """Call out to NLPCloud's create endpoint.

        Args:
            prompt: The prompt to pass into the model.
            stop: Not supported by this interface (pass in init method)

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = nlpcloud("Tell me a joke.")
        """
        response = requests.post(
            self.endpoint,
            json={
                "data": [
                    prompt,
                    self._default_params["max_length"],
                    self._default_params["do_sample"],
                    self._default_params["temperature"],
                    self._default_params["top_p"],
                    self._default_params["typical_p"],
                    self._default_params["repetition_penalty"],
                    self._default_params["encoder_repetition_penalty"],
                    self._default_params["top_k"],
                    self._default_params["min_length"],
                    self._default_params["no_repeat_ngram_size"],
                    self._default_params["num_beams"],
                    self._default_params["penalty_alpha"],
                    self._default_params["length_penalty"],
                    self._default_params["early_stopping"],
                ]
            },
        ).json()
        text = response["data"][0]

        if stop is not None:
            # I believe this is required since the stop tokens
            # are not enforced by the model parameters
            answer = text[len(prompt) - 1 :]

            text = enforce_stop_tokens(answer, stop)

        return text
